{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/","height":986,"referenced_widgets":["40b45c01225e4364a00db1cebdc4560b","c658ebd5011241ca88605d01908e3429","9d0c7557b8ab46fb94904b69fbc21b2c","22bbb3e1c5824779a923c569d51d7fe6","b033bc44e5ca45f4a4e8813ac8506be0","91063b7c198f423e85c13e18453c3fef","5732c72029ce48358ab369f25dfc1ad1","73bc8a0db9064afaa2936173dc6ae14e","5742816494504ac5913d6f20db3e819f","03b79674c4c74b19bd503d7d8da94111","875a20052f53421d981a1455a5e2a80d"]},"executionInfo":{"elapsed":71676,"status":"error","timestamp":1764614234600,"user":{"displayName":"Sasha morgan","userId":"11973072510411566436"},"user_tz":300},"id":"Zs41FRjFpIs-","outputId":"3ae1255a-649f-48b4-89b0-ef8e14d5d0cd"},"outputs":[{"name":"stderr","output_type":"stream","text":["WARNING:torchao.kernel.intmm:Warning: Detected no triton, on systems without Triton certain kernels will not work\n","/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [06:02:58] WARNING: /workspace/src/learner.cc:790: \n","Parameters: { \"use_label_encoder\" } are not used.\n","\n","  bst.update(dtrain, iteration=i, fobj=obj)\n"]},{"name":"stdout","output_type":"stream","text":["Original XGBoost Accuracy: 0.8678932537709265\n","Original MLP Accuracy: 0.8509862423338306\n","Renamed (no fix) XGBoost Accuracy: 0.7512017238521466\n","Renamed (no fix) MLP Accuracy: 0.7512017238521466\n","Renamed + Mapping XGBoost Accuracy: 0.8678932537709265\n","Renamed + Mapping MLP Accuracy: 0.8509862423338306\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:104: UserWarning: \n","Error while fetching `HF_TOKEN` secret value from your vault: 'Requesting secret HF_TOKEN timed out. Secrets can only be fetched when running from the Colab UI.'.\n","You are not authenticated with the Hugging Face Hub in this notebook.\n","If the error persists, please let us know by opening an issue on GitHub (https://github.com/huggingface/huggingface_hub/issues/new).\n","  warnings.warn(\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"40b45c01225e4364a00db1cebdc4560b","version_major":2,"version_minor":0},"text/plain":["modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"c658ebd5011241ca88605d01908e3429","version_major":2,"version_minor":0},"text/plain":["config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"9d0c7557b8ab46fb94904b69fbc21b2c","version_major":2,"version_minor":0},"text/plain":["README.md: 0.00B [00:00, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"22bbb3e1c5824779a923c569d51d7fe6","version_major":2,"version_minor":0},"text/plain":["sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"b033bc44e5ca45f4a4e8813ac8506be0","version_major":2,"version_minor":0},"text/plain":["config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"91063b7c198f423e85c13e18453c3fef","version_major":2,"version_minor":0},"text/plain":["model.safetensors:   0%|          | 0.00/90.9M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"5732c72029ce48358ab369f25dfc1ad1","version_major":2,"version_minor":0},"text/plain":["tokenizer_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"73bc8a0db9064afaa2936173dc6ae14e","version_major":2,"version_minor":0},"text/plain":["vocab.txt: 0.00B [00:00, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"5742816494504ac5913d6f20db3e819f","version_major":2,"version_minor":0},"text/plain":["tokenizer.json: 0.00B [00:00, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"03b79674c4c74b19bd503d7d8da94111","version_major":2,"version_minor":0},"text/plain":["special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"875a20052f53421d981a1455a5e2a80d","version_major":2,"version_minor":0},"text/plain":["config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"ename":"ValueError","evalue":"cannot reindex on an axis with duplicate labels","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m/tmp/ipython-input-353497767.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[0membedding_map\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmatch_columns_by_embedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test_renamed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[0mX_test_emb_mapped\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX_test_renamed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrename\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0membedding_map\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 120\u001b[0;31m \u001b[0mX_test_emb_aligned\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX_test_emb_mapped\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreindex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfill_value\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    121\u001b[0m \u001b[0my_pred_xgb_emb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxgb_clf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test_emb_aligned\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m \u001b[0my_pred_mlp_emb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmlp_clf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test_emb_aligned\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36mreindex\u001b[0;34m(self, labels, index, columns, axis, method, copy, level, fill_value, limit, tolerance)\u001b[0m\n\u001b[1;32m   5376\u001b[0m         \u001b[0mtolerance\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5377\u001b[0m     ) -> DataFrame:\n\u001b[0;32m-> 5378\u001b[0;31m         return super().reindex(\n\u001b[0m\u001b[1;32m   5379\u001b[0m             \u001b[0mlabels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5380\u001b[0m             \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36mreindex\u001b[0;34m(self, labels, index, columns, axis, method, copy, level, fill_value, limit, tolerance)\u001b[0m\n\u001b[1;32m   5608\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5609\u001b[0m         \u001b[0;31m# perform the reindex on the axes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5610\u001b[0;31m         return self._reindex_axes(\n\u001b[0m\u001b[1;32m   5611\u001b[0m             \u001b[0maxes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlimit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtolerance\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfill_value\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5612\u001b[0m         ).__finalize__(self, method=\"reindex\")\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m_reindex_axes\u001b[0;34m(self, axes, level, limit, tolerance, method, fill_value, copy)\u001b[0m\n\u001b[1;32m   5631\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5632\u001b[0m             \u001b[0max\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5633\u001b[0;31m             new_index, indexer = ax.reindex(\n\u001b[0m\u001b[1;32m   5634\u001b[0m                 \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlimit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlimit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtolerance\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtolerance\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5635\u001b[0m             )\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mreindex\u001b[0;34m(self, target, method, level, limit, tolerance)\u001b[0m\n\u001b[1;32m   4427\u001b[0m                 \u001b[0;32melif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_unique\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4428\u001b[0m                     \u001b[0;31m# GH#42568\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4429\u001b[0;31m                     \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"cannot reindex on an axis with duplicate labels\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4430\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4431\u001b[0m                     \u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_indexer_non_unique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: cannot reindex on an axis with duplicate labels"]}],"source":["# Schema Robustness Evaluation Project (with Real Adult Dataset)\n","\n","import pandas as pd\n","import numpy as np\n","import json\n","import xml.etree.ElementTree as ET\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import LabelEncoder\n","from sklearn.metrics import accuracy_score, f1_score\n","import xgboost as xgb\n","from sklearn.neural_network import MLPClassifier\n","from sentence_transformers import SentenceTransformer\n","from sklearn.metrics.pairwise import cosine_similarity\n","from difflib import SequenceMatcher\n","\n","# 1. Load Real Adult Dataset from UCI\n","url = 'https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data'\n","columns = [\n","    'age', 'workclass', 'fnlwgt', 'education', 'education-num', 'marital-status',\n","    'occupation', 'relationship', 'race', 'sex', 'capital-gain', 'capital-loss',\n","    'hours-per-week', 'native-country', 'income']\n","df = pd.read_csv(url, names=columns, sep=r',\\s*', engine='python')\n","\n","# Clean missing data\n","df = df.replace('?', np.nan).dropna()\n","\n","# Drop 'fnlwgt' and 'education-num' for simplicity\n","df = df.drop(columns=['fnlwgt', 'education-num'])\n","\n","# 2. Schema Shift: Rename Columns\n","rename_map = {col: f\"col_{col}\" for col in df.columns if col != 'income'}\n","df_renamed = df.rename(columns=rename_map)\n","\n","# 3. Category Merge Example\n","df['marital-status'] = df['marital-status'].replace({'Divorced': 'Formerly-Married', 'Widowed': 'Formerly-Married'})\n","\n","# 4. Feature Composition Example\n","df['age_bracket'] = pd.cut(df['age'], bins=[0, 30, 50, 100], labels=['Young','Middle','Senior'])\n","\n","# 5. Convert Dataset to JSON and XML (sampled)\n","json_data = []\n","xml_data = []\n","for _, row in df.sample(n=100, random_state=42).iterrows():\n","    rec = row.to_dict()\n","    json_rec = {\n","        \"demographics\": {k: rec[k] for k in ['age','sex','race','marital-status']},\n","        \"education\": rec['education'],\n","        \"work\": {\"workclass\": rec['workclass'], \"hours-per-week\": rec['hours-per-week']},\n","        \"income\": rec['income']\n","    }\n","    json_data.append(json_rec)\n","\n","    root = ET.Element(\"Person\")\n","    dem = ET.SubElement(root, \"Demographics\")\n","    for k in ['age','sex','race','marital-status']:\n","        ET.SubElement(dem, k.replace('-','_')).text = str(rec[k])\n","    wk = ET.SubElement(root, \"Work\")\n","    ET.SubElement(wk, \"workclass\").text = rec['workclass']\n","    ET.SubElement(wk, \"hours_per_week\").text = str(rec['hours-per-week'])\n","    ET.SubElement(root, \"income\").text = rec['income']\n","    xml_data.append(ET.tostring(root).decode())\n","\n","# 6. Structural Similarity Index (SSI)\n","def structural_similarity(json1, json2):\n","    return SequenceMatcher(None, json.dumps(json1, sort_keys=True), json.dumps(json2, sort_keys=True)).ratio()\n","\n","# 7. Key Alignment Accuracy (KAA)\n","def key_alignment(json1, json2):\n","    keys1 = set(json.dumps(json1).replace('{','').replace('}','').split(':'))\n","    keys2 = set(json.dumps(json2).replace('{','').replace('}','').split(':'))\n","    if not keys1: return 0\n","    return len(keys1 & keys2) / len(keys1)\n","\n","# 8. Prepare Train/Test Data\n","le = LabelEncoder()\n","y = le.fit_transform(df['income'])\n","X = pd.get_dummies(df.drop(columns='income'))\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n","\n","# 9. Train Models\n","xgb_clf = xgb.XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=0)\n","xgb_clf.fit(X_train, y_train)\n","mlp_clf = MLPClassifier(hidden_layer_sizes=(16,), max_iter=100, random_state=0)\n","mlp_clf.fit(X_train, y_train)\n","\n","# 10. Evaluate on Original Test\n","y_pred_xgb = xgb_clf.predict(X_test)\n","y_pred_mlp = mlp_clf.predict(X_test)\n","print(\"Original XGBoost Accuracy:\", accuracy_score(y_test, y_pred_xgb))\n","print(\"Original MLP Accuracy:\", accuracy_score(y_test, y_pred_mlp))\n","\n","# 11. Simulate Renamed Schema on Test\n","X_test_renamed = X_test.rename(columns={col: f\"col_{col}\" for col in X_test.columns})\n","X_test_renamed_aligned = X_test_renamed.reindex(columns=X_train.columns, fill_value=0)\n","y_pred_xgb_bad = xgb_clf.predict(X_test_renamed_aligned)\n","y_pred_mlp_bad = mlp_clf.predict(X_test_renamed_aligned)\n","print(\"Renamed (no fix) XGBoost Accuracy:\", accuracy_score(y_test, y_pred_xgb_bad))\n","print(\"Renamed (no fix) MLP Accuracy:\", accuracy_score(y_test, y_pred_mlp_bad))\n","\n","# 12. Dictionary Mapping Fix\n","inv_map = {f\"col_{col}\": col for col in X_train.columns}\n","X_test_mapped = X_test_renamed.rename(columns=inv_map)\n","X_test_mapped_aligned = X_test_mapped.reindex(columns=X_train.columns, fill_value=0)\n","y_pred_xgb_corr = xgb_clf.predict(X_test_mapped_aligned)\n","y_pred_mlp_corr = mlp_clf.predict(X_test_mapped_aligned)\n","print(\"Renamed + Mapping XGBoost Accuracy:\", accuracy_score(y_test, y_pred_xgb_corr))\n","print(\"Renamed + Mapping MLP Accuracy:\", accuracy_score(y_test, y_pred_mlp_corr))\n","\n","# 13. Embedding-Based Fix (Using Sentence-BERT)\n","model = SentenceTransformer('all-MiniLM-L6-v2')\n","def match_columns_by_embedding(renamed_cols, train_cols):\n","    emb_renamed = model.encode(renamed_cols)\n","    emb_train = model.encode(train_cols)\n","    sim_matrix = cosine_similarity(emb_renamed, emb_train)\n","    best_match = sim_matrix.argmax(axis=1)\n","    return {renamed_cols[i]: train_cols[best_match[i]] for i in range(len(renamed_cols))}\n","\n","embedding_map = match_columns_by_embedding(X_test_renamed.columns.tolist(), X_train.columns.tolist())\n","X_test_emb_mapped = X_test_renamed.rename(columns=embedding_map)\n","X_test_emb_aligned = X_test_emb_mapped.reindex(columns=X_train.columns, fill_value=0)\n","y_pred_xgb_emb = xgb_clf.predict(X_test_emb_aligned)\n","y_pred_mlp_emb = mlp_clf.predict(X_test_emb_aligned)\n","print(\"Renamed + Embedding XGBoost Accuracy:\", accuracy_score(y_test, y_pred_xgb_emb))\n","print(\"Renamed + Embedding MLP Accuracy:\", accuracy_score(y_test, y_pred_mlp_emb))\n","\n","# 14. Evaluate Structural Metrics\n","ssi_scores = []\n","kaa_scores = []\n","for i in range(min(50, len(json_data))):\n","    json_shifted = json.loads(json.dumps(json_data[i]).replace('workclass','employment').replace('marital-status','maritalstatus'))\n","    ssi_scores.append(structural_similarity(json_data[i], json_shifted))\n","    kaa_scores.append(key_alignment(json_data[i], json_shifted))\n","\n","print(\"Avg Structural Similarity Index (SSI):\", np.mean(ssi_scores))\n","print(\"Avg Key Alignment Accuracy (KAA):\", np.mean(kaa_scores))"]}],"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOvVoRqnezvaoHtuQGHnhvA"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{}}},"nbformat":4,"nbformat_minor":0}